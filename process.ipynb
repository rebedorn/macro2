{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "import ast\n",
    "import re\n",
    "import itertools\n",
    "from thefuzz import process\n",
    "from itertools import *\n",
    "import neuralcoref\n",
    "import en_core_web_sm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import gender_guesser.detector as gender\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'GenderGapTracker/NLP/main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Extractions\n",
    "The following code shows how to parse for (speaker, organization) with our methods, how to split these results by organization type, and how to extract the most likely gender affiliation for each speaker.\n",
    "\n",
    "### Part 1: Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "from GenderGapTracker.NLP.main.quote_extractor import extract_quotes\n",
    "from GenderGapTracker.NLP.main.entity_gender_annotator import (\n",
    "    merge_nes, remove_invalid_nes, quote_assign\n",
    ")\n",
    "from GenderGapTracker.NLP.main.utils import (\n",
    "    remove_accents, preprocess_text\n",
    ")\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab, max_dist=200)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "\n",
    "# Select functions from GenderTracker file.\n",
    "def collect_quotes(quotes):\n",
    "    \"\"\"Structure final quotes as a list of records for display in a table.\"\"\"\n",
    "    collection = []\n",
    "    for q in quotes:\n",
    "        # Checking for 'PERSON' before assigning a speaker - if the quote is of type 'Heuristic',\n",
    "        # the conditions are relaxed and we accept the quote with a blank speaker name\n",
    "        if q.get('named_entity_type') == 'PERSON' or q.get('quote_type') == 'Heuristic':\n",
    "            speaker = q.get('named_entity', \"\")\n",
    "            quote = preprocess_text(q.get('quote', \"\"))\n",
    "            collection.append({'speaker': speaker, 'quote': quote})\n",
    "    return collection\n",
    "\n",
    "def extract_quotes_and_entities(sample_text):\n",
    "    # \"\"\"Convert raw text to a spaCy doc object and return its named entities and quotes\"\"\"\n",
    "    text = preprocess_text(str(sample_text))\n",
    "    doc = nlp(text)\n",
    "    quotes = extract_quotes(doc_id=\"temp000\", doc=doc, write_tree=False)\n",
    "    unified_nes = merge_nes(doc)\n",
    "    named_entities = remove_invalid_nes(unified_nes)\n",
    "    # Get list of people and sources, along with a combined list of all quotes\n",
    "    people = list(named_entities.keys())\n",
    "    # Obtain gender of speakers from condensed coreference clusters\n",
    "    _, _, all_quotes = quote_assign(named_entities, quotes, doc)\n",
    "    quotes_and_sources = collect_quotes(all_quotes)\n",
    "    # sort alphabetically based on speaker name\n",
    "    quotes_and_sources = sorted(quotes_and_sources, key=lambda x: x['speaker'], reverse=True)\n",
    "    # Get proper list of sources from the list of quotes and speakers\n",
    "    sources = list(set([person['speaker'] for person in quotes_and_sources]))\n",
    "    # Merge list of people and sources (in case there is a mismatch) to get full list of people\n",
    "    people = list(set(people).union(set(sources)))\n",
    "\n",
    "    return people, sources, quotes_and_sources\n",
    "\n",
    "# LIST OF REPORTING VERBS (asserts,declares,says,etc.)\n",
    "reporting_verbs = []\n",
    "with open('GenderGapTracker/NLP/main/rules/quote_verb_list.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        reporting_verbs.append(line[:-1]) # up until newline character\n",
    "        line = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gtracker(df,ents):\n",
    "    qse = pd.DataFrame() # quote speaker entities\n",
    "    missed_speaker = pd.DataFrame()\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(idx)\n",
    "        sp1, sp2, sp3 = ast.literal_eval(str(ents.loc[idx,\"val\"]))\n",
    "        quotes = [d['quote'] for d in sp3]\n",
    "        sents = [sent.text for sent in nlp(row[\"text\"]).sents]\n",
    "        for quote in quotes:\n",
    "            match_sent = [s for s in sents if quote[1:-1] in s]\n",
    "            match_sent = str(match_sent)[1:-1]\n",
    "            doc = nlp(match_sent)\n",
    "            entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "            split_by_quote = str(match_sent).split(quote[1:-1])\n",
    "            context = split_by_quote[0]\n",
    "            doc = nlp(context)\n",
    "            entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "            if 'PERSON' in entities.keys() and 'ORG' in entities.keys():\n",
    "                toappend = [idx, match_sent, entities['PERSON'], entities['ORG'], row[\"source\"]]\n",
    "                qse = qse.append([toappend])\n",
    "            elif 'PERSON' not in entities.keys() and 'ORG' not in entities.keys() and len(split_by_quote) > 1:\n",
    "                context = split_by_quote[1]\n",
    "                doc = nlp(context)\n",
    "                entities = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "                if 'PERSON' in entities.keys() and 'ORG' in entities.keys():\n",
    "                    toappend = [idx, match_sent, entities['PERSON'], entities['ORG'], row[\"source\"]]\n",
    "                    qse = qse.append([toappend])\n",
    "                else:\n",
    "                    toappend = [idx, quote]\n",
    "                    missed_speaker = missed_speaker.append([toappend])\n",
    "            else:\n",
    "                toappend = [idx, quote]\n",
    "                missed_speaker = missed_speaker.append([toappend])\n",
    "    return qse, missed_speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO PROCESS A DATAFRAME\n",
    "def get_qse_ner(df):\n",
    "    print(\"Running GenderTracker Dependency Parse Method...\")\n",
    "    df_ents = pd.DataFrame([[i, extract_quotes_and_entities(df.loc[i,\"text\"])] for i in df.index.values])\n",
    "    print(\"Done.\")\n",
    "    df_ents.columns = ['idx','val']\n",
    "    df_ents.index = df_ents['idx']\n",
    "    print(\"Finding quotes with persons and orgs attached...\")\n",
    "    df_qse, df_missed = parse_gtracker(df,df_ents)\n",
    "    print(\"Done.\")\n",
    "    return (df_qse, df_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GenderTracker Dependency Parse Method...\n",
      "Done.\n",
      "Finding quotes with persons and orgs attached...\n",
      "0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Extract (expert, organization) for dependency parse (DEP Method)\n",
    "aylien_data = pd.read_csv(\"data/data.csv\")\n",
    "aylien_qse, aylien_missed = get_qse_ner(aylien_data[:30])\n",
    "aylien_qse.index = np.arange(0,aylien_qse.shape[0])\n",
    "aylien_qse.columns = ['idx','quote','people','orgs','source']\n",
    "\n",
    "# NER method\n",
    "# find sentences with keyword 'say','says' and 'said'\n",
    "def find_say(s):\n",
    "    index = 0\n",
    "    list_says = []\n",
    "    for i in s:\n",
    "        sentence = Sentence(i)\n",
    "        if(((sentence.to_tagged_string()).find('said') != -1) or ((sentence.to_tagged_string()).find('say') != -1) or\n",
    "          ((sentence.to_tagged_string()).find('says') != -1)):\n",
    "            list_says.append(index)\n",
    "    \n",
    "        index = index + 1\n",
    "    \n",
    "    return list_says"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Distinguishing by Organization Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dataframe softmatching >= thresh% to an organization in org_df\n",
    "# and dataframe of instances which failed to softmatch.\n",
    "def match_org_test(df, org_df, thresh):\n",
    "    matches = pd.DataFrame()\n",
    "    missed = pd.DataFrame()\n",
    "    two = pd.DataFrame()\n",
    "    for idx, match in df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "        score = 0\n",
    "        for t_org in ast.literal_eval(str(match['orgs'])):\n",
    "            t_org = t_org.replace(\"\\\\\",\"\")\n",
    "            # Skip organizations 1 or 2 chars long\n",
    "            if len(t_org) <= 2:\n",
    "                continue\n",
    "            thisorg, thisscore = process.extract(t_org, org_df, limit=1)[0]\n",
    "            if thisscore > score:\n",
    "                org = thisorg\n",
    "                score = thisscore\n",
    "        if score >= thresh:\n",
    "            toappend = [idx, match['idx'], match['quote'], match['people'], org]\n",
    "            matches = matches.append([toappend])\n",
    "        else:\n",
    "            missed = missed.append(match)\n",
    "    return (matches, missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split by organization type\n",
    "# Read in educational institutions and match where we can\n",
    "\n",
    "# DEP Method\n",
    "times15 = pd.read_csv('data/timesData-2015rankings.csv')\n",
    "educ_qse, missed = match_org_test(aylien_qse,times15.university_name.unique(),90)\n",
    "educ_qse = educ_qse.drop(columns = [0])\n",
    "educ_qse.columns = ['idx','quote','people','educ_inst']\n",
    "educ_qse.index = np.arange(educ_qse.shape[0])\n",
    "\n",
    "gov = pd.read_csv(\"data/gov_agencies.csv\")\n",
    "gov_qse, missed = match_org_test(missed,gov[\"0\"].unique(),90)\n",
    "gov_qse = gov_qse.drop(columns = [0])\n",
    "gov_qse.columns = ['idx','quote','people','orgs']\n",
    "gov_qse.index = np.arange(gov_qse.shape[0])\n",
    "\n",
    "thinktanks = pd.read_csv(\"data/thinktanks.csv\")\n",
    "thinkt_qse, missed = match_org_test(missed, thinktanks[\"tt_name_en\"].unique(),90)\n",
    "thinkt_qse.columns = ['idx','quote','people','orgs']\n",
    "thinkt_qse.index = np.arange(thinkt_qse.shape[0])\n",
    "\n",
    "# NER Method\n",
    "titles = []\n",
    "dates = []\n",
    "sources = []\n",
    "orgs = []\n",
    "experts = []\n",
    "pairs = []\n",
    "sens = []\n",
    "\n",
    "ans = []\n",
    "\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from nltk import tokenize\n",
    "from flair.models import SequenceTagger\n",
    "# NER processing\n",
    "for i in range(len(aylien_data)):\n",
    "    source = aylien_data.iloc[i]['source']\n",
    "    date = aylien_data.iloc[i]['date']\n",
    "    title = aylien_data.iloc[i]['title']\n",
    "    text = aylien_data.iloc[i]['text']\n",
    "    sen1 = tokenize.sent_tokenize(text)\n",
    "    model = SequenceTagger.load('ner-ontonotes-fast') #.load('ner')\n",
    "    res1 = find_say(sen1)\n",
    "    for n in res1:\n",
    "        sentence = Sentence(sen1[n])\n",
    "        model.predict(sentence)\n",
    "        flair = sentence.to_dict(tag_type='ner')\n",
    "        sen = sen1[n]\n",
    "        experts = []\n",
    "        orgs = []\n",
    "        for i in flair['entities']:\n",
    "            print(i)\n",
    "            p1 = ''\n",
    "            org1 = ''\n",
    "            skip = 0\n",
    "            if('PERSON' in str(i['labels'][0])):\n",
    "                p1 = i['text']\n",
    "            #print(p1)\n",
    "            elif('ORG' in str(i['labels'][0])):\n",
    "                org1 = i['text']\n",
    "            else:\n",
    "                skip = 1\n",
    "            #print(org1)\n",
    "        # add a new entry or not\n",
    "            if(skip == 0):\n",
    "                if(p1 != ''):\n",
    "                    experts.append(p1)\n",
    "                if(org1 != ''):\n",
    "                    orgs.append(org1)\n",
    "        ans.append([experts, orgs, source, date, title, [sen]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in ans:\n",
    "    if(i[1] != [] and i[0] != []):\n",
    "        result.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate items in ans and clean up duplicates\n",
    "sep = []\n",
    "dup = []\n",
    "\n",
    "for x in result:\n",
    "    if(len(x[0]) > 1):\n",
    "        dup.append(x)\n",
    "        for i in range(len(x[0])):\n",
    "            #print(len(x))\n",
    "            name = x[0][i]\n",
    "            #print(\"name:  \")\n",
    "            #print([name])\n",
    "            copy = [name, x[1], x[2], x[3], x[4], x[5]]\n",
    "            #print(\"copy:  \")\n",
    "            print(copy)\n",
    "            sep.append(copy)\n",
    "\n",
    "for i in dup:\n",
    "    result.remove(i)\n",
    "\n",
    "for n in sep:\n",
    "    result.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export processed data in df\n",
    "df = pd.DataFrame(result,columns=['Name','Org','Source','Date', 'Title', 'Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 'Count' for each 'Org'\n",
    "freq = df.groupby('Org').size()\n",
    "org_count = df[['Org']]\n",
    "org_count = pd.DataFrame(freq, columns=['Count'])\n",
    "org_count = org_count.sort_values(by=\"Count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_count = org_count.values.tolist()\n",
    "names = times15['university_name'].to_list()\n",
    "ranks = times15['world_rank'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "# soft matching universities\n",
    "ranks_uni = []\n",
    "for i in uni_count:\n",
    "    for x in range(len(names)):\n",
    "        if(fuzz.ratio(i[0],names[x]) > 83):\n",
    "            temp1 = i.append(ranks[x])\n",
    "            ranks_uni.append([i[1],names[x],ranks[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "for i in uni_count:\n",
    "    process.extract(i[0], names, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "ranks = []\n",
    "universities = []\n",
    "for i in ranks_uni:\n",
    "    counts.append(i[0])\n",
    "    universities.append(i[1])\n",
    "    if(len(i[2]) > 3):\n",
    "        ranks.append(int(i[2][0:3]))\n",
    "    else:\n",
    "        ranks.append(int(i[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(universities)):\n",
    "    if(universities[i] not in result):\n",
    "        result.append([counts[i], universities[i], ranks[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'count' merge for same org\n",
    "df = pd.DataFrame(result,columns=['count','name','ranking'])\n",
    "df['counts'] = df.groupby(['name', 'ranking'])['count'].transform('sum')\n",
    "# drop duplicates\n",
    "new_df = df.drop_duplicates(subset=['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Gender Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in andy_dict.csv and unknown_dict.csv\n",
    "unknown = pd.read_csv(\"data/unknown_dict.csv\")\n",
    "unknown = unknown.loc[:,['Person','Gender']]\n",
    "unknown['Person'] = unknown['Person'].str.replace('\"','')\n",
    "unknown['Gender'] = unknown['Gender'].str.replace('\"','')\n",
    "unknown_dict = dict([(p,g) for p,g in zip(unknown.Person,unknown.Gender)])\n",
    "\n",
    "andy = pd.read_csv(\"data/andy_dict.csv\")\n",
    "andy = andy.loc[:,['Person','Gender']]\n",
    "andy['Person'] = andy['Person'].str.replace('\"','')\n",
    "andy['Gender'] = andy['Gender'].str.replace('\"','')\n",
    "andy_dict = dict([(p,g) for p,g in zip(andy.Person,andy.Gender)])\n",
    "\n",
    "def get_genders_total(df, usedict = True):\n",
    "    d = gender.Detector()\n",
    "    male = 0; female = 0; unknown = 0; andy = 0\n",
    "    gender_var = {\"male\" : male, \"female\": female, \"unknown\": unknown, \"andy\" : andy}\n",
    "    df[\"gender_t\"] = 0\n",
    "    df[\"male_t\"] = 0\n",
    "    df[\"female_t\"] = 0\n",
    "    df[\"unknown_t\"] = 0\n",
    "    df[\"andy_t\"] = 0\n",
    "    for i, row in df.iterrows():\n",
    "        row_gender = []\n",
    "        male = 0; female = 0; unknown = 0; andy = 0\n",
    "        for person in ast.literal_eval(row[\"people\"]):\n",
    "            p = person.split(\" \")[0]\n",
    "            if p in unknown_dict.keys() and usedict == True:\n",
    "                g = unknown_dict[p]\n",
    "            elif p in andy_dict.keys() and usedict == True:\n",
    "                g = andy_dict[p]\n",
    "            else:\n",
    "                g = d.get_gender(p)\n",
    "                if g == \"mostly_male\":\n",
    "                    g = \"male\"\n",
    "                elif g == \"mostly_female\":\n",
    "                    g = \"female\"\n",
    "            row_gender.append(g)\n",
    "            gender_var[g] += 1\n",
    "            df.loc[i,str(g)+\"_t\"] += 1\n",
    "        df.loc[i,\"gender_t\"] = str(row_gender)\n",
    "    return df\n",
    "\n",
    "def get_genders_unique(df):\n",
    "    d = gender.Detector()\n",
    "    male = 0; female = 0; unknown = 0; andy = 0\n",
    "    gender_var = {\"male\" : male, \"female\": female, \"unknown\": unknown, \"andy\" : andy}\n",
    "    df[\"gender_u\"] = 0\n",
    "    df[\"male_u\"] = 0\n",
    "    df[\"female_u\"] = 0\n",
    "    df[\"unknown_u\"] = 0\n",
    "    df[\"andy_u\"] = 0\n",
    "    past = pd.DataFrame(columns=['Name', 'Count', 'Gender'])\n",
    "    for i, row in df.iterrows():\n",
    "        row_gender = []\n",
    "        male = 0; female = 0; unknown = 0; andy = 0\n",
    "        for person in ast.literal_eval(row[\"people\"]):\n",
    "            p = person.split(\" \")[0]\n",
    "            trip = process.extract(p,past['Name'],limit=1)\n",
    "            if trip:\n",
    "                person, score, num = trip[0]\n",
    "                if score > 90:\n",
    "                    past.loc[past[past[\"Name\"] == person].index.values[0]][\"Count\"] = past.at[past[past[\"Name\"] == person].index.values[0],\"Count\"] + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if p in unknown_dict.keys():\n",
    "                        g = unknown_dict[p]\n",
    "                    elif p in andy_dict.keys():\n",
    "                        g = andy_dict[p]\n",
    "                    else:\n",
    "                        g = d.get_gender(p)\n",
    "                        if g == \"mostly_male\":\n",
    "                            g = \"male\"\n",
    "                        elif g == \"mostly_female\":\n",
    "                            g = \"female\"\n",
    "                    row_gender.append(g)\n",
    "                    gender_var[g] += 1\n",
    "                    df.loc[i,str(g)+\"_u\"] += 1\n",
    "                    past = past.append(pd.DataFrame([[p,1,g]],columns=[\"Name\",\"Count\",\"Gender\"]))\n",
    "            else:\n",
    "                if p in unknown_dict.keys():\n",
    "                    g = unknown_dict[p]\n",
    "                elif p in andy_dict.keys():\n",
    "                    g = andy_dict[p]\n",
    "                else:\n",
    "                    g = d.get_gender(p)\n",
    "                    if g == \"mostly_male\":\n",
    "                        g = \"male\"\n",
    "                    elif g == \"mostly_female\":\n",
    "                        g = \"female\"\n",
    "                row_gender.append(g)\n",
    "                gender_var[g] += 1\n",
    "                df.loc[i,str(g)+\"_u\"] += 1\n",
    "                past = pd.DataFrame([[p,1,g]],columns=[\"Name\",\"Count\",\"Gender\"])\n",
    "            df.loc[i,\"gender_u\"] = str(row_gender)\n",
    "    return (df, past)\n",
    "\n",
    "\n",
    "# Creates four new columns in df that signal the number of people quoted\n",
    "# with that gender\n",
    "def gender_counts_total(df):\n",
    "    df['male'] = 0; df['female'] = 0; df['unknown'] = 0; df['andy'] = 0\n",
    "    gender_dict = {'male' : 0, 'mostly_male': 0, 'female': 1, 'mostly_female': 1, 'andy' : 2, 'unknown' : 3} #[0,1,2,3] = [male,female,andy,unknown]\n",
    "    rev_gender_dict = {0:'male', 1:'female', 2:'andy', 3:'unknown'}\n",
    "    past = pd.DataFrame(columns=['Names','Count','Gender'])\n",
    "    for i, row in df.iterrows():\n",
    "        genders = ast.literal_eval(row['gender'])\n",
    "        people = ast.literal_eval(row['people'])\n",
    "        for tup in zip(people,genders):\n",
    "            p, g = tup\n",
    "            p = re.sub(r'[(]', '', p)\n",
    "            p = re.sub(r'[)]', '', p)\n",
    "            trip = process.extract(p, past['Names'],limit=1)\n",
    "            if g == 'unknown':\n",
    "                trip = process.extract(p,past['Names'],limit=1)\n",
    "                if trip:\n",
    "                    closestperson, score, num = trip[0]\n",
    "                    if score >= 90:\n",
    "                        closestgender = past.loc[past[\"Names\"] == closestperson][\"Gender\"]\n",
    "                        df.loc[i,rev_gender_dict[closestgender.values[0]]] += 1\n",
    "                    else:\n",
    "                        df.loc[i,'unknown'] += 1\n",
    "                else:\n",
    "                    df.loc[i,'unknown'] += 1\n",
    "            else:\n",
    "                if g == 'mostly_male':\n",
    "                    df.loc[i,'male'] += 1\n",
    "                elif g == 'mostly_female':\n",
    "                    df.loc[i,'female'] += 1\n",
    "                else:\n",
    "                    df.loc[i,g] += 1\n",
    "                past = past.append(pd.DataFrame([[p, gender_dict[g]]],columns=['Names', 'Gender'],index=[i]))\n",
    "    return df, past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aylien_qse.people = aylien_qse.people.astype(str)\n",
    "\n",
    "# DEP\n",
    "whole_qse = get_genders_total(aylien_qse)\n",
    "whole_educ = get_genders_total(educ_qse)\n",
    "whole_gov = get_genders_total(gov_qse)\n",
    "whole_thinkt = get_genders_total(thinkt_qse)\n",
    "\n",
    "# NER\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "total = 0\n",
    "whole_educ_clean = whole_educ\n",
    "times15 = pd.read_csv('/Users/am/Desktop/aylien_covid_news_data/usnews_health_uni_ranking.csv')\n",
    "times15[\"c_total\"] = 0; times15[\"c_breit\"] = 0; times15[\"c_nyp\"] = 0; times15[\"c_fox\"] = 0;\n",
    "times15[\"c_cnn\"] = 0; times15[\"c_nyt\"] = 0; times15[\"c_huff\"] = 0# introduce column to count university mentions\n",
    "\n",
    "times_new = times15[times15['name'] != 'Pennsylvania State University']\n",
    "\n",
    "for i, x in times_new.iterrows():\n",
    "    univ = x['name']\n",
    "    total = 0\n",
    "    for l in [\"BREIT\",\"NYP\",\"FOX\",\"CNN\",\"NYT\",\"HUFF\"]:\n",
    "        df_slice = whole_educ_clean[whole_educ_clean['outlet'] == l]\n",
    "        df_slice = df_slice[df_slice['educ_inst'].str.contains(univ)]\n",
    "        times_new.loc[i,\"c_\"+l.lower()] = df_slice.shape[0]\n",
    "        total += df_slice.shape[0]\n",
    "    times_new.loc[i,\"c_total\"] = total\n",
    "\n",
    "def gini(x):\n",
    "    total = 0\n",
    "    for i, xi in enumerate(x):\n",
    "        total += np.sum(np.abs(xi - x[i:]))\n",
    "    return total / (len(x)**2 * np.mean(x))\n",
    "\n",
    "print(\"WITH ZERO COUNTS\")\n",
    "rank_cit = pd.DataFrame([times_new.index.values,times_new.c_huff.values]).T\n",
    "rank_cit.columns = ['rank_', 'citations']\n",
    "print(\"Spearman corr: \",scipy.stats.spearmanr(rank_cit.rank_.values,rank_cit.citations.values))\n",
    "print(\"Gini: \",gini(np.array(list(itertools.chain.from_iterable([[i]*x.citations for i, x in rank_cit.iterrows()])))))\n",
    "print(\"Pearson corr: \",scipy.stats.pearsonr(rank_cit.rank_.values,rank_cit.citations.values))\n",
    "\n",
    "rank_cit = rank_cit[rank_cit.citations > 0]\n",
    "print(\"WITHOUT ZERO COUNTS\")\n",
    "print(\"Spearman corr: \",scipy.stats.spearmanr(rank_cit.rank_.values,rank_cit.citations.values))\n",
    "rank_cit\n",
    "print(\"Gini: \",gini(np.array(list(itertools.chain.from_iterable([[i]*x.citations for i, x in rank_cit.iterrows()])))))\n",
    "print(\"Pearson corr:\",pearsonr(rank_cit.rank_.values,rank_cit.citations))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
